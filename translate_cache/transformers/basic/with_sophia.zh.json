{
 "<h1>Transformer Auto-Regression Experiment with <a href=\"../../optimizers/sophia.html\">Sophia-G optimizer</a></h1>\n<p>This trains a simple transformer introduced in <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a> on an NLP auto-regression task (with Tiny Shakespeare dataset) with <a href=\"../../optimizers/sophia.html\">Sophia-G optimizer</a>.</p>\n": "<h1>Transformer Auto-Regression Experiment with <a href=\"../../optimizers/sophia.html\">Sophia-G optimizer</a></h1>\n<p>This trains a simple transformer introduced in <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a> on an NLP auto-regression task (with Tiny Shakespeare dataset) with <a href=\"../../optimizers/sophia.html\">Sophia-G optimizer</a>.</p>\n",
 "<h2>Configurations</h2>\n<p>This inherits from <a href=\"autoregressive_experiment.html\"><span translate=no>_^_0_^_</span></a></p>\n": "<h2>Configurations</h2>\n<p>This inherits from <a href=\"autoregressive_experiment.html\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h3>Training or validation step with Gauss-Newton-Bartlett (GNB) Hessian diagonal estimator</h3>\n": "<h3>Training or validation step with Gauss-Newton-Bartlett (GNB) Hessian diagonal estimator</h3>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p>Batch size <span translate=no>_^_0_^_</span> </p>\n": "<p>Batch size <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Calculate and log accuracy </p>\n": "<p>Calculate and log accuracy </p>\n",
 "<p>Calculate and log loss </p>\n": "<p>Calculate and log loss </p>\n",
 "<p>Calculate gradients </p>\n": "<p>Calculate gradients </p>\n",
 "<p>Clear the gradients </p>\n": "<p>Clear the gradients </p>\n",
 "<p>Clip gradients </p>\n": "<p>Clip gradients </p>\n",
 "<p>Create a categorical distribution from logits </p>\n": "<p>Create a categorical distribution from logits </p>\n",
 "<p>Create configs </p>\n": "<p>Create configs </p>\n",
 "<p>Create experiment </p>\n": "<p>Create experiment </p>\n",
 "<p>Estimate the Hessian diagonal every <span translate=no>_^_0_^_</span> steps </p>\n": "<p>Estimate the Hessian diagonal every <span translate=no>_^_0_^_</span> steps </p>\n",
 "<p>Get model outputs </p>\n": "<p>Get model outputs </p>\n",
 "<p>Get model outputs. It&#x27;s returning a tuple for states when using RNNs. This is not implemented yet. \ud83d\ude1c </p>\n": "<p>Get model outputs. It&#x27;s returning a tuple for states when using RNNs. This is not implemented yet. \ud83d\ude1c </p>\n",
 "<p>Log the model parameters and gradients on last batch of every epoch </p>\n": "<p>Log the model parameters and gradients on last batch of every epoch </p>\n",
 "<p>Model size </p>\n": "<p>Model size </p>\n",
 "<p>Move data to the device </p>\n": "<p>Move data to the device </p>\n",
 "<p>Override configurations </p>\n": "<p>Override configurations </p>\n",
 "<p>Prompt separator is blank </p>\n": "<p>Prompt separator is blank </p>\n",
 "<p>Run training </p>\n": "<p>Run training </p>\n",
 "<p>Sample <span translate=no>_^_0_^_</span> </p>\n": "<p>Sample <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Save the tracked metrics </p>\n": "<p>Save the tracked metrics </p>\n",
 "<p>Set models for saving and loading </p>\n": "<p>Set models for saving and loading </p>\n",
 "<p>Set training/eval mode </p>\n": "<p>Set training/eval mode </p>\n",
 "<p>Start the experiment </p>\n": "<p>Start the experiment </p>\n",
 "<p>Starting prompt for sampling </p>\n": "<p>Starting prompt for sampling </p>\n",
 "<p>Switch between training and validation for <span translate=no>_^_0_^_</span> times per epoch </p>\n": "<p>Switch between training and validation for <span translate=no>_^_0_^_</span> times per epoch </p>\n",
 "<p>Take optimizer step </p>\n": "<p>Take optimizer step </p>\n",
 "<p>Train for 32 epochs </p>\n": "<p>Train for 32 epochs </p>\n",
 "<p>Train the model </p>\n": "<p>Train the model </p>\n",
 "<p>Update EMA Hessian diagonal</p>\n<span translate=no>_^_0_^_</span><p> </p>\n": "<p>Update EMA Hessian diagonal</p>\n<span translate=no>_^_0_^_</span><p> </p>\n",
 "<p>Update global step (number of tokens processed) when in training mode </p>\n": "<p>Update global step (number of tokens processed) when in training mode </p>\n",
 "<p>Use <a href=\"../../optimizers/sophia.html\">Sophia optimizer</a> </p>\n": "<p>Use <a href=\"../../optimizers/sophia.html\">Sophia optimizer</a> </p>\n",
 "<p>Use Tiny Shakespeare dataset </p>\n": "<p>Use Tiny Shakespeare dataset </p>\n",
 "<p>Use a context size of <span translate=no>_^_0_^_</span> </p>\n": "<p>Use a context size of <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Use character level tokenizer </p>\n": "<p>Use character level tokenizer </p>\n",
 "<p>Whether to capture model outputs </p>\n": "<p>Whether to capture model outputs </p>\n",
 "This trains a simple transformer model on NLP auto-regression with Sophia-G optimizer.": "This trains a simple transformer model on NLP auto-regression with Sophia-G optimizer.",
 "Transformer Auto-Regression Experiment with [Sophia-G optimizer](../../optimizers/sophia.html)": "Transformer Auto-Regression Experiment with [Sophia-G optimizer](../../optimizers/sophia.html)"
}