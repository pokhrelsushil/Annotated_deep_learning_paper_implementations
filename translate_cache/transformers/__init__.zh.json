{
 "<h1>Transformers</h1>\n": "<h1>Transformers</h1>\n",
 "<h2><a href=\"aft/index.html\">Attention Free Transformer</a></h2>\n": "<h2><a href=\"aft/index.html\">\u65e0\u6ce8\u610f\u529b Transformer</a></h2>\n",
 "<h2><a href=\"alibi/index.html\">Attention with Linear Biases</a></h2>\n": "<h2><a href=\"alibi/index.html\">\u7ebf\u6027\u504f\u5dee\u6ce8\u610f\u529b</a></h2>\n",
 "<h2><a href=\"compressive/index.html\">Compressive Transformer</a></h2>\n": "<h2><a href=\"compressive/index.html\">\u538b\u7f29 Transformer</a></h2>\n",
 "<h2><a href=\"fast_weights/index.html\">Fast Weights Transformer</a></h2>\n": "<h2><a href=\"fast_weights/index.html\">\u5feb\u901f\u6743\u91cd Transformer</a></h2>\n",
 "<h2><a href=\"feedback/index.html\">Feedback Transformer</a></h2>\n": "<h2><a href=\"feedback/index.html\">\u81ea\u53cd\u9988 Transformer</a></h2>\n",
 "<h2><a href=\"fnet/index.html\">FNet: Mixing Tokens with Fourier Transforms</a></h2>\n": "<h2><a href=\"fnet/index.html\">Fnet\uff1a\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\u6df7\u5408 token </a></h2>\n",
 "<h2><a href=\"glu_variants/simple.html\">GLU Variants</a></h2>\n": "<h2><a href=\"glu_variants/simple.html\">GLU \u53d8\u4f53</a></h2>\n",
 "<h2><a href=\"gmlp/index.html\">Pay Attention to MLPs (gMLP)</a></h2>\n": "<h2><a href=\"gmlp/index.html\">\u95e8\u63a7\u591a\u5c42\u611f\u77e5\u5668 (gMLP)</a></h2>\n",
 "<h2><a href=\"gpt/index.html\">GPT Architecture</a></h2>\n": "<h2><a href=\"gpt/index.html\">GPT \u67b6\u6784</a></h2>\n",
 "<h2><a href=\"hour_glass/index.html\">Hourglass</a></h2>\n": "<h2><a href=\"hour_glass/index.html\">\u6c99\u6f0f\u7f51\u7edc</a></h2>\n",
 "<h2><a href=\"knn/index.html\">kNN-LM</a></h2>\n": "<h2><a href=\"knn/index.html\">kNN-LM</a></h2>\n",
 "<h2><a href=\"mlm/index.html\">Masked Language Model</a></h2>\n": "<h2><a href=\"mlm/index.html\">\u63a9\u7801\u8bed\u8a00\u6a21\u578b</a></h2>\n",
 "<h2><a href=\"mlp_mixer/index.html\">MLP-Mixer: An all-MLP Architecture for Vision</a></h2>\n": "<h2><a href=\"mlp_mixer/index.html\">MLP-Mixer\uff1a\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u7684\u5168 MLP \u67b6\u6784</a></h2>\n",
 "<h2><a href=\"primer_ez/index.html\">Primer EZ</a></h2>\n": "<h2><a href=\"primer_ez/index.html\">Primer</a></h2>\n",
 "<h2><a href=\"retro/index.html\">RETRO</a></h2>\n": "<h2><a href=\"retro/index.html\">RETRO</a></h2>\n",
 "<h2><a href=\"rope/index.html\">Rotary Positional Embeddings</a></h2>\n": "<h2><a href=\"rope/index.html\">\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801</a></h2>\n",
 "<h2><a href=\"switch/index.html\">Switch Transformer</a></h2>\n": "<h2><a href=\"switch/index.html\">Switch Transformer</a></h2>\n",
 "<h2><a href=\"vit/index.html\">Vision Transformer (ViT)</a></h2>\n": "<h2><a href=\"vit/index.html\">\u89c6\u89c9 Transformer (ViT)</a></h2>\n",
 "<h2><a href=\"xl/index.html\">Transformer XL</a></h2>\n": "<h2><a href=\"xl/index.html\">Transformer XL</a></h2>\n",
 "<p>This implements Attention with Linear Biases (ALiBi).</p>\n": "<p>\u8fd9\u662f\u7ebf\u6027\u504f\u5dee\u6ce8\u610f\u529b\uff08 ALIBI \uff09\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This implements Rotary Positional Embeddings (RoPE)</p>\n": "<p>\u8fd9\u662f\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801\uff08 ROPE \uff09\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This implements Transformer XL model using <a href=\"xl/relative_mha.html\">relative multi-head attention</a></p>\n": "<p>\u8fd9\u662f\u4f7f\u7528<a href=\"xl/relative_mha.html\">\u76f8\u5bf9\u591a\u5934\u6ce8\u610f\u529b</a>\u7684 Transformer XL \u6a21\u578b\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This implements the Retrieval-Enhanced Transformer (RETRO).</p>\n": "<p>\u8fd9\u662f\u5bf9\u68c0\u7d22\u589e\u5f3a Transformer \uff08 RETRO \uff09\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is a miniature implementation of the paper <a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>. Our implementation only has a few million parameters and doesn&#x27;t do model parallel distributed training. It does single GPU training but we implement the concept of switching as described in the paper.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2101.03961\">\u300a Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity \u300b</a>\u7684\u4e00\u4e2a\u7b80\u5316\u5b9e\u73b0\u3002\u6211\u4eec\u7684\u5b9e\u73b0\u4ec5\u5305\u542b\u51e0\u767e\u4e07\u4e2a\u53c2\u6570\uff0c\u5e76\u4e14\u53ea\u5728\u5355 GPU \u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e0d\u6d89\u53ca\u5e76\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u4f46\u6211\u4eec\u4ecd\u7136\u5b9e\u73b0\u4e86\u8bba\u6587\u4e2d\u63cf\u8ff0\u7684 Switch \u6982\u5ff5\u3002</p>\n",
 "<p>This is an implementation of GPT-2 architecture.</p>\n": "<p>\u8fd9\u662f GPT-2 \u7ed3\u6784\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of Masked Language Model used for pre-training in paper <a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/1810.04805\">\u300a BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u300b</a>\u4e2d\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u73b0</p>\n",
 "<p>This is an implementation of compressive transformer that extends upon <a href=\"xl/index.html\">Transformer XL</a> by compressing the oldest memories to give a longer attention span.</p>\n": "<p>\u8fd9\u662f\u4e00\u4e2a\u538b\u7f29transformer\u7684\u5b9e\u73b0\uff0c\u5b83\u5728<a href=\"xl/index.html\">Transformer XL</a> \u7684\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u538b\u7f29\u6700\u65e9\u671f\u7684\u8bb0\u5fc6\u6765\u5ef6\u957f\u6ce8\u610f\u529b\u8de8\u5ea6\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/1911.00172\">Generalization through Memorization: Nearest Neighbor Language Models</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/1911.00172\">\u300a Generalization through Memorization: Nearest Neighbor Language Models \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2002.05202\">GLU Variants Improve Transformer</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587 <a href=\"https://arxiv.org/abs/2002.05202\">\u300a GLU Variants Improve Transformer \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2002.09402\">Accessing Higher-level Representations in Sequential Transformers with Feedback Memory</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2002.09402\">\u300a Accessing Higher-level Representations in Sequential Transformers with Feedback Memory \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2010.11929\">An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2010.11929\">\u300a An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2102.11174\">Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587 <a href=\"https://arxiv.org/abs/2102.11174\">\u300a Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.01601\">MLP-Mixer: An all-MLP Architecture for Vision</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587 <a href=\"https://arxiv.org/abs/2105.01601\">\u300a MLP-Mixer: An all-MLP Architecture for Vision \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.03824\">FNet: Mixing Tokens with Fourier Transforms</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2105.03824\">\u300a FNet: Mixing Tokens with Fourier Transforms \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.08050\">Pay Attention to MLPs</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2105.08050\">\u300a Pay Attention to MLPs \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2105.14103\">An Attention Free Transformer</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2105.14103\">\u300a An Attention Free Transformer \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2109.08668\">\u300a Primer: Searching for Efficient Transformers for Language Modeling \u300b</a>\u7684\u5b9e\u73b0\u3002</p>\n",
 "<p>This is an implementation of the paper <a href=\"https://arxiv.org/abs/2110.13711\">Hierarchical Transformers Are More Efficient Language Models</a></p>\n": "<p>\u8fd9\u662f\u8bba\u6587<a href=\"https://arxiv.org/abs/2110.13711\">\u300a Hierarchical Transformers Are More Efficient Language Models \u300b</a>\u7684\u5b9e\u73b0</p>\n",
 "<p>This module contains <a href=\"https://pytorch.org/\">PyTorch</a> implementations and explanations of original transformer from paper <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, and derivatives and enhancements of it.</p>\n": "</a><p>\u672c\u8282\u5185\u5bb9\u5305\u542b\u5bf9\u8bba\u6587<a href=\"https://arxiv.org/abs/1706.03762\">\u300a Attention is All You Need \u300b</a>\u4e2d\u539f\u59cb Transformer \u7684\u89e3\u91ca\u4e0e<a href=\"https://pytorch.org/\">PyTorch</a> \u5b9e\u73b0\uff0c\u4ee5\u53ca\u5bf9\u5176\u884d\u751f\u548c\u589e\u5f3a\u7248\u672c\u7684\u89e3\u91ca\u4e0e\u5b9e\u73b0\u3002</p>\n",
 "<ul><li><a href=\"mha.html\">Multi-head attention</a> </li>\n<li><a href=\"models.html\">Transformer Encoder and Decoder Models</a> </li>\n<li><a href=\"feed_forward.html\">Position-wise Feed Forward Network (FFN)</a> </li>\n<li><a href=\"positional_encoding.html\">Fixed positional encoding</a></li></ul>\n": "<ul><li><a href=\"mha.html\">\u591a\u5934\u6ce8\u610f\u529b</a></li>\n<li><a href=\"models.html\">Transformer \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b</a></li>\n<li><a href=\"feed_forward.html\">\u4f4d\u7f6e\u524d\u9988\u7f51\u7edc (FFN)</a></li>\n<li><a href=\"positional_encoding.html\">\u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801</a></li></ul>\n",
 "This is a collection of PyTorch implementations/tutorials of transformers and related techniques.": "\u8fd9\u662f\u4e00\u4e2a\u5305\u542b Transformers \u53ca\u76f8\u5173\u6280\u672f\u7684 PyTorch \u5b9e\u73b0\u548c\u6559\u7a0b\u7684\u5408\u96c6\u3002",
 "Transformers": "Transformers"
}